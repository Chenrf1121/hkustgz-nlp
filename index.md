---
layout: home
---

## Course Description

This course provides students with a deep understanding of how computational systems process and generate human languages. The curriculum bridges foundational linguistic theories with cutting-edge deep learning techniques, moving from classic N-gram models and Hidden Markov Models to modern Transformer architectures and Large Language Models (LLMs). The course explores critical topics such as tokenization, embeddings, syntactic parsing, and machine translation, while also diving into advanced contemporary subjects like Reinforcement Learning from Human Feedback (RLHF), Retrieval-Augmented Generation (RAG), and model compression. Assessments include in-class quizzes, a closed-book final exam, one homework, one individual coding project, and a significant final team research project that includes a poster presentation and technical report. Students will gain the expertise necessary to build, evaluate, and analyze sophisticated NLP systems capable of addressing real-world challenges in the rapidly evolving AI landscape.

**Syllabus**: [Download PDF](/hkustgz-nlp/static_files/syllabus.pdf)

## Course Overview

### Topics Covered

- **Foundational Concepts**: Language modeling, N-Grams, tokenization, and word embeddings
- **Classical Models**: Hidden Markov Models (HMM), syntactic trees, and parsing
- **Neural Architectures**: Recurrent Neural Networks (RNNs), attention mechanisms, and Transformers
- **Advanced Architectures**: Mixture of Experts models
- **Model Training**: Pretraining, Supervised Fine-Tuning (SFT), and LoRA
- **Advanced Training Techniques**: Reinforcement Learning from Human Feedback (RLHF), PPO, and DPO
- **Contemporary Topics**: Training data synthetization, Retrieval-Augmented Generation (RAG), scaling laws
- **Optimization & Efficiency**: KV cache, model compression
- **Generative Models**: Diffusion models for NLP applications

### Assessments

- **In-class Quizzes (24%)**: Closed-book quizzes in the last 5 minutes of each class
- **Homework (5%)**: One homework assignment
- **Individual Coding Project (10%)**: Individual coding project using PyTorch
- **Final Team Research Project (35%)**: Team project (2-3 members) with poster presentation and technical report
- **Final Exam (26%)**: Closed-book written exam covering all lecture materials

### Prerequisites

- Basic understanding of machine learning and AI concepts
- Programming experience in Python
- Familiarity with linear algebra and probability
- Experience with PyTorch is helpful but not required

For course announcements and updates, please check Canvas and this website regularly.