---
layout: home
---

**Syllabus**: [Download PDF](/hkustgz-nlp/static_files/syllabus.pdf)

## Course Overview

### Topics Covered

- **Foundational Concepts**: Language modeling, N-Grams, tokenization, and word embeddings
- **Classical Models**: Hidden Markov Models (HMM), syntactic trees, and parsing
- **Neural Architectures**: Recurrent Neural Networks (RNNs), attention mechanisms, and Transformers
- **Advanced Architectures**: Mixture of Experts models
- **Model Training**: Pretraining, Supervised Fine-Tuning (SFT), and LoRA
- **Advanced Training Techniques**: Reinforcement Learning from Human Feedback (RLHF), PPO, and DPO
- **Contemporary Topics**: Training data synthetization, Retrieval-Augmented Generation (RAG), scaling laws
- **Optimization & Efficiency**: KV cache, model compression
- **Generative Models**: Diffusion models for NLP applications

### Assessments

- **In-class Quizzes (24%)**: Closed-book quizzes in the last 5 minutes of each class
- **Homework (5%)**: One homework assignment
- **Individual Coding Project (10%)**: Individual coding project using PyTorch
- **Final Team Research Project (35%)**: Team project (2-3 members) with poster presentation and technical report
- **Final Exam (26%)**: Closed-book written exam covering all lecture materials

### Prerequisites

- Basic understanding of machine learning and AI concepts
- Programming experience in Python
- Familiarity with linear algebra and probability
- Experience with PyTorch is helpful but not required

For course announcements and updates, please check Canvas and this website regularly.